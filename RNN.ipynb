{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6c8cHr5Ppao"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBegMzj_K-Jn"
      },
      "outputs": [],
      "source": [
        "class Rnn:\n",
        "    def __init__(self, input_size, hidden_state_size, output_size):\n",
        "      # input_size = the number of values (dimensions) in each embedding vector.\n",
        "      # For example, a one-hot encoding like [1, 0, 0, 0, 0] can represent a word by its position in the vocabulary,\n",
        "      # but it doesn't capture the meaning or similarity between words.\n",
        "      # In contrast, an embedding vector like [0.1, 0.6, 0.4] represents the same word in a way that captures its semantic meaning.\n",
        "      # So two similar words may have similar embeddings, like [0.1, 0.6, 0.4] and [0.11, 0.58, 0.43], indicating they are related in meaning.\n",
        "\n",
        "\n",
        "        # hidden_state_size = size of the hidden state vector h_t 128 is a good start for medium levels of complexity\n",
        "        # output_size = size of the vocabulary (we're predicting next word from vocab)\n",
        "\n",
        "        # Weights to map input (embedding) to hidden state\n",
        "        self.W_xh = np.random.randn(hidden_state_size, input_size) * 0.01\n",
        "\n",
        "        # Weights to map previous hidden state to next hidden state\n",
        "        self.W_hh = np.random.randn(hidden_state_size, hidden_state_size) * 0.01\n",
        "\n",
        "        # Weights to map hidden state to output logits (vocab-sized)\n",
        "        self.W_hy = np.random.randn(output_size, hidden_state_size) * 0.01\n",
        "\n",
        "        # Initial hidden state (starts as zeros)\n",
        "        self.h = np.zeros((hidden_state_size, 1))\n",
        "\n",
        "        # hidden state for each phase\n",
        "        self.h_phases = [np.zeros_like(self.h)]\n",
        "        self.y_phases = []\n",
        "        self.x_phases = []\n",
        "\n",
        "        # Embedding matrix: each row is a word vector, basically a dictionary of all our words in our vocabulary.\n",
        "        self.E = np.random.randn(output_size, input_size) * 0.01\n",
        "\n",
        "        # biases needed for better learning, basically extra level of rules that\n",
        "        # contain extra information about when a neuron should fire that isn't contained in the weights.\n",
        "        # In other words, they encode additional rules or tendencies for when a neuron should \"fire\"\n",
        "        # that aren't captured by the weights alone.\n",
        "        self.b_h = np.zeros((hidden_state_size, 1))\n",
        "        self.b_y = np.zeros((output_size, 1))\n",
        "\n",
        "\n",
        "    def embed_word(self, word_index): # the word index can be in this case: What index of the words one hot encoding is equal to 1 while the rest are zero. eg [0, 0, 1, 0] means we are looking for index  2\n",
        "        return self.E[word_index].reshape(-1, 1) # basically turning an array into a one column vector.\n",
        "\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x))  # Stabilizing to prevent overflow\n",
        "        return exp_x / np.sum(exp_x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, self.h) + self.b_h)\n",
        "        self.h_phases.append(self.h.copy())\n",
        "        y = np.dot(self.W_hy, self.h) + self.b_y\n",
        "        output = self.softmax(y)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # the true seq is of shape T,V where T is the number of steps in overall sentence and V is the size of bag of words.\n",
        "  # basically something like [[001], [100], [010]]\n",
        "    def learn(self, trueSeq, alpha, inputSeq, number_of_iterations):\n",
        "        for i in range(number_of_iterations):\n",
        "          loss = 0\n",
        "          for t in range(len(trueSeq)):\n",
        "            word = inputSeq[t] # could also be character in one hot encoding\n",
        "            embed_idx = np.argmax(word)\n",
        "            x = self.embed_word(embed_idx)\n",
        "            self.x_phases.append(x.copy())\n",
        "            y = self.forward(x)\n",
        "            self.y_phases.append(y.copy())\n",
        "            target_idx = np.argmax(trueSeq[t]) # use argmax to get the idx of the 1 at \"should be out\" one hot encoding\n",
        "            loss += -np.log(y[target_idx]) # use that index to get the prob predicted for that word in the output, use negative log likelyhood loss function\n",
        "          mean_loss = loss / len(trueSeq)\n",
        "          self.backpropagate(alpha, trueSeq, inputSeq)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def backpropagate(self, alpha, true_seq, input_seq):\n",
        "        hh_acc = np.zeros_like(self.W_hh)\n",
        "        xh_acc = np.zeros_like(self.W_xh)\n",
        "        hy_acc = np.zeros_like(self.W_hy)\n",
        "        bh_acc = np.zeros_like(self.b_h)\n",
        "        by_acc = np.zeros_like(self.b_y)\n",
        "        e_acc = np.zeros_like(self.E)\n",
        "\n",
        "\n",
        "\n",
        "        # this was the tricky to understand. when propagating the error signals to the hidden state at time step t, remember to propagate the error from the future hidden state also.\n",
        "        dh_next = np.zeros_like(self.h)\n",
        "\n",
        "        # start from the last output,\n",
        "        # accumulate the gradients of the hidden layer.\n",
        "        for idx in reversed(range(len(true_seq))):\n",
        "          y_pred = self.y_phases[idx] # the last most output\n",
        "          y_true = true_seq[idx] # the last most true seq\n",
        "          dy = y_pred - y_true # the gradient of the output\n",
        "          by_acc += dy # accumulate the gradient of the bias\n",
        "          ht = self.h_phases[idx+1]\n",
        "          x_t = self.x_phases[idx]\n",
        "          ht_prev = self.h_phases[idx]\n",
        "          dht = np.dot(self.W_hy.T, dy) * (1 - ht**2) + dh_next\n",
        "          dh_next = dht\n",
        "          hh_acc += np.dot(dht, ht_prev.T)\n",
        "          hy_acc += np.dot(dy, ht.T)\n",
        "          bh_acc += dht\n",
        "          xh_acc += np.dot(dht, x_t.T)\n",
        "          dx = np.dot(self.W_xh.T, dht)\n",
        "          embed_index = np.argmax(input_seq[idx])\n",
        "          e_acc[embed_index] += dx.flatten()\n",
        "          # clear\n",
        "\n",
        "        self.W_xh -= alpha * xh_acc\n",
        "        self.W_hh -= alpha * hh_acc\n",
        "        self.W_hy -= alpha * hy_acc\n",
        "        self.b_h -= alpha * bh_acc\n",
        "        self.b_y -= alpha * by_acc\n",
        "        self.E -= alpha * e_acc\n",
        "\n",
        "        # Clear phases\n",
        "        self.h_phases = [np.zeros_like(self.h)]\n",
        "        self.y_phases = []\n",
        "        self.x_phases = []\n",
        "\n",
        "    def save(self, filename=\"rnn_model.pkl\"):\n",
        "      model_data = {\n",
        "          \"W_xh\": self.W_xh,\n",
        "          \"W_hh\": self.W_hh,\n",
        "          \"W_hy\": self.W_hy,\n",
        "          \"b_h\": self.b_h,\n",
        "          \"b_y\": self.b_y,\n",
        "          \"E\": self.E,\n",
        "          \"h\": self.h,\n",
        "          \"input_size\": self.W_xh.shape[1],\n",
        "          \"hidden_state_size\": self.W_xh.shape[0],\n",
        "          \"output_size\": self.W_hy.shape[0]\n",
        "      }\n",
        "      with open(filename, \"wb\") as f:\n",
        "        pickle.dump(model_data, f)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(filename=\"rnn_model.pkl\"):\n",
        "      with open(filename, \"rb\") as f:\n",
        "        model_data = pickle.load(f)\n",
        "\n",
        "      rnn = Rnn(model_data[\"input_size\"], model_data[\"hidden_state_size\"], model_data[\"output_size\"])\n",
        "      rnn.W_xh = model_data[\"W_xh\"]\n",
        "      rnn.W_hh = model_data[\"W_hh\"]\n",
        "      rnn.W_hy = model_data[\"W_hy\"]\n",
        "      rnn.b_h = model_data[\"b_h\"]\n",
        "      rnn.b_y = model_data[\"b_y\"]\n",
        "      rnn.E = model_data[\"E\"]\n",
        "      rnn.h = model_data[\"h\"]\n",
        "      return rnn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPMyCGTv8eXs",
        "outputId": "e87ff405-631d-4d72-96fa-ab040094ad15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGO6swBRB4qJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "drive_path = '/content/drive/MyDrive/stored_weights'\n",
        "if not os.path.exists(drive_path):\n",
        "  os.makedirs(drive_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbi-LjYtXJB_"
      },
      "source": [
        "Got tired of trying to find a dataset for python code, will just create mine using github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9_ENilkXOUt",
        "outputId": "d7941c5d-ab3a-47ba-93db-a3aa201a3eb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'requests'...\n",
            "remote: Enumerating objects: 26209, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 26209 (delta 47), reused 31 (delta 31), pack-reused 26133 (from 3)\u001b[K\n",
            "Receiving objects: 100% (26209/26209), 12.88 MiB | 34.45 MiB/s, done.\n",
            "Resolving deltas: 100% (17176/17176), done.\n",
            "Repository cloned!\n",
            "drive/\trequests/  sample_data/\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/psf/requests.git\n",
        "print(\"Repository cloned!\")\n",
        "!ls -F # Verify 'requests/' directory exists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnX1aW_NXnO5",
        "outputId": "3a46291b-f79a-48b8-bad4-949087c2d3ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting Python files from 'requests' and saving to 'requests_code_dataset.txt'...\n",
            "\n",
            "Dataset created: 'requests_code_dataset.txt'\n",
            "Total size of generated dataset file: 0.36 MB\n",
            "Total characters in the loaded dataset: 378324\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "codebase_root_dir = 'requests'\n",
        "output_dataset_file = 'requests_code_dataset.txt'\n",
        "\n",
        "print(f\"Collecting Python files from '{codebase_root_dir}' and saving to '{output_dataset_file}'...\")\n",
        "\n",
        "with open(output_dataset_file, 'w', encoding='utf-8') as outfile:\n",
        "    # os.walk generates the file names in a directory tree by walking the tree\n",
        "    for dirpath, _, filenames in os.walk(codebase_root_dir):\n",
        "        for f in filenames:\n",
        "            if f.endswith('.py'):\n",
        "                file_path = os.path.join(dirpath, f)\n",
        "                try:\n",
        "                    with open(file_path, 'r', encoding='utf-8') as infile:\n",
        "                        outfile.write(infile.read())\n",
        "                        outfile.write(\"\\n\\n# --- FILE_SEPARATOR ---\\n\\n\")\n",
        "                except UnicodeDecodeError:\n",
        "                    print(f\"Skipping {file_path} due to encoding error (likely non-UTF-8 characters).\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Could not read {file_path}: {e}\")\n",
        "\n",
        "print(f\"\\nDataset created: '{output_dataset_file}'\")\n",
        "print(f\"Total size of generated dataset file: {os.path.getsize(output_dataset_file) / (1024*1024):.2f} MB\")\n",
        "\n",
        "# Read the generated dataset into your 'text' variable for the RNN\n",
        "with open(output_dataset_file, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f\"Total characters in the loaded dataset: {len(text)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vs0kQEB1wOE7",
        "outputId": "e9731deb-897a-4684-a5f2-980ee6768f26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "378324\n"
          ]
        }
      ],
      "source": [
        "print(len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJ0yQ_DiwS2F"
      },
      "outputs": [],
      "source": [
        "text = text[:50000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyF0pRy0ac2Y",
        "outputId": "afe80357-ca09-414d-c4f6-40973cb659c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 88, characters: \n",
            " !\"#'()*+,-./0123456789:<=>ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_`abcdefghijklmnopqrstuvwxyz{|}~\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
        "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "print(f\"Vocabulary size: {vocab_size}, characters: {''.join(chars)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fU7Cy1FObek4",
        "outputId": "b7d89875-55f3-4587-9e3a-eec28c9dd0a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, \"'\": 5, '(': 6, ')': 7, '*': 8, '+': 9, ',': 10, '-': 11, '.': 12, '/': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22, '9': 23, ':': 24, '<': 25, '=': 26, '>': 27, 'A': 28, 'B': 29, 'C': 30, 'D': 31, 'E': 32, 'F': 33, 'G': 34, 'H': 35, 'I': 36, 'J': 37, 'K': 38, 'L': 39, 'M': 40, 'N': 41, 'O': 42, 'P': 43, 'Q': 44, 'R': 45, 'S': 46, 'T': 47, 'U': 48, 'V': 49, 'W': 50, 'X': 51, 'Y': 52, 'Z': 53, '[': 54, ']': 55, '_': 56, '`': 57, 'a': 58, 'b': 59, 'c': 60, 'd': 61, 'e': 62, 'f': 63, 'g': 64, 'h': 65, 'i': 66, 'j': 67, 'k': 68, 'l': 69, 'm': 70, 'n': 71, 'o': 72, 'p': 73, 'q': 74, 'r': 75, 's': 76, 't': 77, 'u': 78, 'v': 79, 'w': 80, 'x': 81, 'y': 82, 'z': 83, '{': 84, '|': 85, '}': 86, '~': 87}\n"
          ]
        }
      ],
      "source": [
        "print(char_to_ix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmJU6AR0eaj3"
      },
      "outputs": [],
      "source": [
        "input_size = 128\n",
        "hidden_state_size = 128\n",
        "output_size = vocab_size\n",
        "rnn = Rnn(input_size, hidden_state_size, output_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-ECbSIIem0A"
      },
      "outputs": [],
      "source": [
        "seq_length = 30       # Number of characters in each training sequence chunk\n",
        "learning_rate = 0.01\n",
        "num_epochs = 100     # Number of times to iterate during training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9wxd34jie8aE"
      },
      "outputs": [],
      "source": [
        "# will crash your ram.\n",
        "\n",
        "# training_data_pairs = []\n",
        "# for i in range(0, len(text) - seq_length):\n",
        "#     input_chunk_chars = text[i : i + seq_length]\n",
        "#     target_chunk_chars = text[i + 1 : i + seq_length + 1] # Shifted by one for next char prediction\n",
        "\n",
        "#     input_one_hots_sequence = []\n",
        "#     target_one_hots_sequence = []\n",
        "\n",
        "#     for char_in, char_target in zip(input_chunk_chars, target_chunk_chars):\n",
        "#         input_one_hot_seq = np.zeros((vocab_size, 1))\n",
        "#         input_one_hot_seq[char_to_ix[char_in]] = 1\n",
        "#         input_one_hots_sequence.append(input_one_hot_seq)\n",
        "#         target_one_hot_seq = np.zeros((vocab_size,1))\n",
        "#         target_one_hot_seq[char_to_ix[char_target]] = 1\n",
        "#         target_one_hots_sequence.append(target_one_hot_seq)\n",
        "\n",
        "#     training_data_pairs.append((input_one_hots_sequence, target_one_hots_sequence))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwI0PtjXtHGS",
        "outputId": "0b008965-6709-42a4-845c-ad41a930312c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Training with Generator ---\n",
            "Epoch 1/100\n",
            "Epoch 2/100\n",
            "Epoch 3/100\n",
            "Epoch 4/100\n",
            "Epoch 5/100\n",
            "Epoch 6/100\n",
            "Epoch 7/100\n",
            "Epoch 8/100\n",
            "Epoch 9/100\n",
            "Epoch 10/100\n",
            "Epoch 11/100\n",
            "Epoch 12/100\n",
            "Epoch 13/100\n",
            "Epoch 14/100\n",
            "Epoch 15/100\n",
            "Epoch 16/100\n",
            "Epoch 17/100\n",
            "Epoch 18/100\n",
            "Epoch 19/100\n",
            "Epoch 20/100\n",
            "Epoch 21/100\n",
            "Epoch 22/100\n",
            "Epoch 23/100\n",
            "Epoch 24/100\n",
            "Epoch 25/100\n",
            "Epoch 26/100\n",
            "Epoch 27/100\n",
            "Epoch 28/100\n",
            "Epoch 29/100\n",
            "Epoch 30/100\n",
            "Epoch 31/100\n",
            "Epoch 32/100\n",
            "Epoch 33/100\n",
            "Epoch 34/100\n",
            "Epoch 35/100\n",
            "Epoch 36/100\n",
            "Epoch 37/100\n",
            "Epoch 38/100\n",
            "Epoch 39/100\n",
            "Epoch 40/100\n",
            "Epoch 41/100\n",
            "Epoch 42/100\n",
            "Epoch 43/100\n",
            "Epoch 44/100\n",
            "Epoch 45/100\n",
            "Epoch 46/100\n",
            "Epoch 47/100\n",
            "Epoch 48/100\n",
            "Epoch 49/100\n",
            "Epoch 50/100\n",
            "Epoch 51/100\n",
            "Epoch 52/100\n",
            "Epoch 53/100\n",
            "Epoch 54/100\n",
            "Epoch 55/100\n",
            "Epoch 56/100\n",
            "Epoch 57/100\n",
            "Epoch 58/100\n",
            "Epoch 59/100\n",
            "Epoch 60/100\n",
            "Epoch 61/100\n",
            "Epoch 62/100\n",
            "Epoch 63/100\n",
            "Epoch 64/100\n",
            "Epoch 65/100\n",
            "Epoch 66/100\n",
            "Epoch 67/100\n",
            "Epoch 68/100\n",
            "Epoch 69/100\n",
            "Epoch 70/100\n",
            "Epoch 71/100\n",
            "Epoch 72/100\n",
            "Epoch 73/100\n",
            "Epoch 74/100\n",
            "Epoch 75/100\n",
            "Epoch 76/100\n",
            "Epoch 77/100\n",
            "Epoch 78/100\n",
            "Epoch 79/100\n",
            "Epoch 80/100\n",
            "Epoch 81/100\n",
            "Epoch 82/100\n",
            "Epoch 83/100\n",
            "Epoch 84/100\n",
            "Epoch 85/100\n",
            "Epoch 86/100\n",
            "Epoch 87/100\n",
            "Epoch 88/100\n",
            "Epoch 89/100\n",
            "Epoch 90/100\n",
            "Epoch 91/100\n",
            "Epoch 92/100\n",
            "Epoch 93/100\n",
            "Epoch 94/100\n",
            "Epoch 95/100\n",
            "Epoch 96/100\n",
            "Epoch 97/100\n",
            "Epoch 98/100\n",
            "Epoch 99/100\n",
            "Epoch 100/100\n",
            "\n",
            "--- Model Saved ---\n"
          ]
        }
      ],
      "source": [
        "# --- Generator Function ---\n",
        "def create_training_data_generator(text_data, sequence_length, char_to_ix, vocab_size):\n",
        "    for i in range(0, len(text_data) - sequence_length):\n",
        "        input_chunk_chars = text_data[i : i + sequence_length]\n",
        "        target_chunk_chars = text_data[i + 1 : i + sequence_length + 1]\n",
        "\n",
        "        input_one_hots_sequence = []\n",
        "        target_one_hots_sequence = []\n",
        "\n",
        "        for char_in, char_target in zip(input_chunk_chars, target_chunk_chars):\n",
        "            input_oh = np.zeros((vocab_size, 1))\n",
        "            input_oh[char_to_ix[char_in]] = 1\n",
        "            input_one_hots_sequence.append(input_oh)\n",
        "\n",
        "            target_oh = np.zeros((vocab_size, 1))\n",
        "            target_oh[char_to_ix[char_target]] = 1\n",
        "            target_one_hots_sequence.append(target_oh)\n",
        "\n",
        "        yield (input_one_hots_sequence, target_one_hots_sequence) # Use yield instead of append\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n--- Starting Training with Generator ---\")\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    data_generator_obj = create_training_data_generator(text, sequence_length=seq_length, char_to_ix=char_to_ix, vocab_size=vocab_size)\n",
        "\n",
        "    for seq_idx, (input_seq_data, target_seq_data) in enumerate(data_generator_obj):\n",
        "        rnn.learn(trueSeq=target_seq_data, alpha=learning_rate, inputSeq=input_seq_data, number_of_iterations=1)\n",
        "\n",
        "\n",
        "# file path /content/drive/MyDrive/stored_weights\n",
        "\n",
        "rnn.save(filename=\"/content/drive/MyDrive/stored_weights/rnn_model.pkl\")\n",
        "print(\"\\n--- Model Saved ---\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbH1-NcbJmSk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "406c51f8-d022-4b62-d5cb-e186052f2b55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.47364226e+00]\n",
            " [-1.09431793e+00]\n",
            " [ 3.40232921e-01]\n",
            " [-1.40906374e-01]\n",
            " [ 1.23059877e+00]\n",
            " [-7.25153688e-01]\n",
            " [ 7.35200790e-01]\n",
            " [ 1.01625939e+00]\n",
            " [ 5.46533465e-01]\n",
            " [ 4.32076454e-02]\n",
            " [ 2.03124209e-01]\n",
            " [ 1.22528051e-01]\n",
            " [-8.48052135e-01]\n",
            " [ 8.09301265e-02]\n",
            " [ 8.03299653e-02]\n",
            " [ 1.16373468e+00]\n",
            " [ 5.92278781e-01]\n",
            " [-1.47456544e-01]\n",
            " [ 4.67511876e-02]\n",
            " [ 8.08959579e-01]\n",
            " [ 7.94666804e-01]\n",
            " [ 5.94058559e-01]\n",
            " [ 8.48466126e-01]\n",
            " [-5.68788894e-01]\n",
            " [-9.42082061e-01]\n",
            " [-1.03751687e+00]\n",
            " [ 2.05638072e-01]\n",
            " [-1.83878903e-01]\n",
            " [ 4.57862630e-01]\n",
            " [-3.18673577e-01]\n",
            " [ 1.45847764e+00]\n",
            " [-4.44595851e-01]\n",
            " [ 8.63609814e-01]\n",
            " [ 1.20412452e+00]\n",
            " [-5.51603199e-02]\n",
            " [-6.98758519e-01]\n",
            " [-2.07781873e-01]\n",
            " [-1.02951478e-01]\n",
            " [-5.44832860e-01]\n",
            " [ 4.70798468e-01]\n",
            " [ 6.80829672e-01]\n",
            " [-7.66087784e-01]\n",
            " [ 1.52405163e-03]\n",
            " [-1.10237421e-01]\n",
            " [ 4.25543437e-01]\n",
            " [-1.58604543e-01]\n",
            " [ 7.96624856e-01]\n",
            " [ 8.57150461e-01]\n",
            " [ 2.30290682e-01]\n",
            " [-2.03187149e-01]\n",
            " [ 5.09902362e-01]\n",
            " [-1.58479446e+00]\n",
            " [-5.46505043e-01]\n",
            " [ 5.64341729e-01]\n",
            " [-1.66404882e-01]\n",
            " [-1.06397227e+00]\n",
            " [ 1.79456995e-02]\n",
            " [-6.83479969e-01]\n",
            " [ 7.09743728e-01]\n",
            " [-5.59961337e-01]\n",
            " [ 6.71795443e-01]\n",
            " [-2.19577015e-01]\n",
            " [-1.05925442e+00]\n",
            " [-1.86397020e-01]\n",
            " [-1.11364946e+00]\n",
            " [-1.14179085e+00]\n",
            " [ 4.39086063e-01]\n",
            " [ 3.14296280e-01]\n",
            " [-4.54969545e-02]\n",
            " [ 4.45147026e-01]\n",
            " [ 1.26878603e+00]\n",
            " [-6.69787934e-01]\n",
            " [-6.21295450e-01]\n",
            " [-7.91133599e-01]\n",
            " [ 9.31749854e-01]\n",
            " [ 1.37370711e+00]\n",
            " [ 1.85248889e-01]\n",
            " [ 3.20536903e-01]\n",
            " [-4.88466275e-04]\n",
            " [-1.02598990e+00]\n",
            " [ 7.73777174e-01]\n",
            " [ 5.37253232e-01]\n",
            " [-1.37599274e+00]\n",
            " [-3.30069852e-01]\n",
            " [-7.63622815e-01]\n",
            " [-9.60517792e-01]\n",
            " [ 1.88261705e-01]\n",
            " [-4.96259813e-01]\n",
            " [ 1.27806413e+00]\n",
            " [-1.18122210e+00]\n",
            " [-9.62248046e-01]\n",
            " [-5.23644718e-01]\n",
            " [ 5.84985094e-01]\n",
            " [ 6.00951743e-01]\n",
            " [ 5.03319132e-01]\n",
            " [ 3.33770684e-01]\n",
            " [ 1.15659194e+00]\n",
            " [-7.83893635e-01]\n",
            " [ 7.38125005e-01]\n",
            " [ 4.01422069e-01]\n",
            " [-6.44790771e-01]\n",
            " [-9.87943910e-01]\n",
            " [ 1.87326924e-01]\n",
            " [ 2.62663683e-01]\n",
            " [-2.75447700e-02]\n",
            " [ 7.00262738e-01]\n",
            " [-8.75491332e-01]\n",
            " [ 6.68577150e-01]\n",
            " [-8.59958116e-01]\n",
            " [-3.48239781e-01]\n",
            " [ 6.06158054e-01]\n",
            " [-3.20548448e-01]\n",
            " [-1.03841481e+00]\n",
            " [-4.74433954e-01]\n",
            " [-3.39601215e-01]\n",
            " [-1.24596862e+00]\n",
            " [-8.91589170e-01]\n",
            " [ 3.00686895e-01]\n",
            " [-1.20930314e+00]\n",
            " [ 6.80561422e-01]\n",
            " [-5.65083018e-03]\n",
            " [-6.57103586e-01]\n",
            " [ 1.11911540e+00]\n",
            " [-1.90442049e-01]\n",
            " [-7.61731563e-01]\n",
            " [ 3.09141578e-01]\n",
            " [-4.46651101e-01]\n",
            " [ 3.93771855e-01]]\n"
          ]
        }
      ],
      "source": [
        "# load a pkl file\n",
        "\n",
        "rnn_loaded = Rnn.load(filename=\"/content/drive/MyDrive/stored_weights/rnn_model.pkl\")\n",
        "print(rnn_loaded.b_h)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"rnn_loaded.W_xh (first few values):\", rnn_loaded.W_xh.flatten()[:5])\n",
        "print(\"rnn_loaded.W_hh (first few values):\", rnn_loaded.W_hh.flatten()[:5])\n",
        "print(\"rnn_loaded.W_hy (first few values):\", rnn_loaded.W_hy.flatten()[:5])\n",
        "print(\"rnn_loaded.b_h (first few values):\", rnn_loaded.b_h.flatten()[:5])\n",
        "print(\"rnn_loaded.b_y (first few values):\", rnn_loaded.b_y.flatten()[:5])\n",
        "print(\"rnn_loaded.h (first few values):\", rnn_loaded.h.flatten()[:5]) # Check initial hidden state too"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpbvHIoIaKxJ",
        "outputId": "7023f880-22a4-4f23-9aef-ae7d6bdbb6fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rnn_loaded.W_xh (first few values): [ 0.01371443  0.00103356  0.01878313 -0.01171187  0.00974692]\n",
            "rnn_loaded.W_hh (first few values): [ 0.48141423  0.23056956  0.12433143  0.05371233 -0.65925096]\n",
            "rnn_loaded.W_hy (first few values): [-0.20104251 -0.2146878  -0.025979    0.12006784  0.30567001]\n",
            "rnn_loaded.b_h (first few values): [-1.47364226 -1.09431793  0.34023292 -0.14090637  1.23059877]\n",
            "rnn_loaded.b_y (first few values): [ 0.50421542  0.4427166  -0.16886216 -0.08671471 -0.16269076]\n",
            "rnn_loaded.h (first few values): [-1. -1.  1. -1.  1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM6R-lPQj64V"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, seed_text, num_chars_to_generate, char_to_ix, ix_to_char, hidden_size):\n",
        "    generated_text_chars = list(seed_text)\n",
        "    model.h = np.zeros((hidden_size, 1), dtype=np.float32 if hasattr(model, 'W_hh') and model.W_hh.dtype == np.float32 else np.float64)\n",
        "    for char_in_seed in seed_text:\n",
        "        idx_in = char_to_ix.get(char_in_seed, 0)\n",
        "        x = model.embed_word(idx_in)\n",
        "        _ = model.forward(x)\n",
        "    if seed_text:\n",
        "        last_char_idx = char_to_ix.get(seed_text[-1], 0)\n",
        "    else:\n",
        "        last_char_idx = char_to_ix.get(' ', 0) if ' ' in char_to_ix else 0\n",
        "    for _ in range(num_chars_to_generate):\n",
        "        x = model.embed_word(last_char_idx)\n",
        "        output_probs = model.forward(x)\n",
        "        p = output_probs.ravel()\n",
        "        p /= p.sum()\n",
        "\n",
        "        next_char_idx = np.random.choice(len(p), p=p)\n",
        "\n",
        "        next_char = ix_to_char[next_char_idx]\n",
        "        generated_text_chars.append(next_char)\n",
        "\n",
        "        last_char_idx = next_char_idx\n",
        "\n",
        "    return \"\".join(generated_text_chars)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JrXFZMdkj5t",
        "outputId": "6f183ded-33f0-4e81-ba8f-26eaea46e26a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generating text with seed: 'def my_function(self, arg):' ---\n",
            "def my_function(self, arg):                    \n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Generated text with seed: 'import numpy as ':\n",
            "import numpy as                                                                                                                                                       \n",
            "Generated text with seed: '    ':\n",
            "                                                                                                        \n"
          ]
        }
      ],
      "source": [
        "seed_text_example = \"def my_function(self, arg):\"\n",
        "num_chars_to_generate_example = 20\n",
        "\n",
        "print(f\"\\n--- Generating text with seed: '{seed_text_example}' ---\")\n",
        "generated_code = generate_text(\n",
        "    model=rnn_loaded,\n",
        "    seed_text=seed_text_example,\n",
        "    num_chars_to_generate=num_chars_to_generate_example,\n",
        "    char_to_ix=char_to_ix,\n",
        "    ix_to_char=ix_to_char,\n",
        "    hidden_size=hidden_state_size\n",
        ")\n",
        "\n",
        "print(generated_code)\n",
        "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "\n",
        "seed_text_example_2 = \"import numpy as \"\n",
        "generated_code_2 = generate_text(rnn, seed_text_example_2, 150, char_to_ix, ix_to_char, hidden_size=hidden_state_size)\n",
        "print(f\"Generated text with seed: '{seed_text_example_2}':\\n{generated_code_2}\")\n",
        "\n",
        "\n",
        "seed_text_example_3 = \"    \"\n",
        "generated_code_3 = generate_text(rnn, seed_text_example_3, 100, char_to_ix, ix_to_char, hidden_size=hidden_state_size)\n",
        "print(f\"Generated text with seed: '{seed_text_example_3}':\\n{generated_code_3}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPbuy4UY9QlS6YdjH5h6dZB"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}