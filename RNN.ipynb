{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6c8cHr5Ppao"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBegMzj_K-Jn"
      },
      "outputs": [],
      "source": [
        "class Rnn:\n",
        "    def __init__(self, input_size, hidden_state_size, output_size):\n",
        "      # input_size = the number of values (dimensions) in each embedding vector.\n",
        "      # For example, a one-hot encoding like [1, 0, 0, 0, 0] can represent a word by its position in the vocabulary,\n",
        "      # but it doesn't capture the meaning or similarity between words.\n",
        "      # In contrast, an embedding vector like [0.1, 0.6, 0.4] represents the same word in a way that captures its semantic meaning.\n",
        "      # So two similar words may have similar embeddings, like [0.1, 0.6, 0.4] and [0.11, 0.58, 0.43], indicating they are related in meaning.\n",
        "\n",
        "\n",
        "        # hidden_state_size = size of the hidden state vector h_t 128 is a good start for medium levels of complexity\n",
        "        # output_size = size of the vocabulary (we're predicting next word from vocab)\n",
        "\n",
        "        # Weights to map input (embedding) to hidden state\n",
        "        self.W_xh = np.random.randn(hidden_state_size, input_size) * 0.01\n",
        "\n",
        "        # Weights to map previous hidden state to next hidden state\n",
        "        self.W_hh = np.random.randn(hidden_state_size, hidden_state_size) * 0.01\n",
        "\n",
        "        # Weights to map hidden state to output logits (vocab-sized)\n",
        "        self.W_hy = np.random.randn(output_size, hidden_state_size) * 0.01\n",
        "\n",
        "        # Initial hidden state (starts as zeros)\n",
        "        self.h = np.zeros((hidden_state_size, 1))\n",
        "\n",
        "        # hidden state for each phase\n",
        "        self.h_phases = [np.zeros_like(self.h)]\n",
        "        self.y_phases = []\n",
        "        self.x_phases = []\n",
        "\n",
        "        # Embedding matrix: each row is a word vector, basically a dictionary of all our words in our vocabulary.\n",
        "        self.E = np.random.randn(output_size, input_size) * 0.01\n",
        "\n",
        "        # biases needed for better learning, basically extra level of rules that\n",
        "        # contain extra information about when a neuron should fire that isn't contained in the weights.\n",
        "        # In other words, they encode additional rules or tendencies for when a neuron should \"fire\"\n",
        "        # that aren't captured by the weights alone.\n",
        "        self.b_h = np.zeros((hidden_state_size, 1))\n",
        "        self.b_y = np.zeros((output_size, 1))\n",
        "\n",
        "\n",
        "    def embed_word(self, word_index): # the word index can be in this case: What index of the words one hot encoding is equal to 1 while the rest are zero. eg [0, 0, 1, 0] means we are looking for index  2\n",
        "        return self.E[word_index].reshape(-1, 1) # basically turning an array into a one column vector.\n",
        "\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x))  # Stabilizing to prevent overflow\n",
        "        return exp_x / np.sum(exp_x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, self.h) + self.b_h)\n",
        "        self.h_phases.append(self.h.copy())\n",
        "        y = np.dot(self.W_hy, self.h) + self.b_y\n",
        "        output = self.softmax(y)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # the true seq is of shape T,V where T is the number of steps in overall sentence and V is the size of bag of words.\n",
        "  # basically something like [[001], [100], [010]]\n",
        "    def learn(self, trueSeq, alpha, inputSeq, number_of_iterations):\n",
        "        for i in range(number_of_iterations):\n",
        "          loss = 0\n",
        "          for t in range(len(trueSeq)):\n",
        "            word = inputSeq[t] # could also be character in one hot encoding\n",
        "            embed_idx = np.argmax(word)\n",
        "            x = self.embed_word(embed_idx)\n",
        "            self.x_phases.append(x.copy())\n",
        "            y = self.forward(x)\n",
        "            self.y_phases.append(y.copy())\n",
        "            target_idx = np.argmax(trueSeq[t]) # use argmax to get the idx of the 1 at \"should be out\" one hot encoding\n",
        "            loss += -np.log(y[target_idx]) # use that index to get the prob predicted for that word in the output, use negative log likelyhood loss function\n",
        "          mean_loss = loss / len(trueSeq)\n",
        "          self.backpropagate(alpha, trueSeq, inputSeq)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def backpropagate(self, alpha, true_seq, input_seq):\n",
        "        hh_acc = np.zeros_like(self.W_hh)\n",
        "        xh_acc = np.zeros_like(self.W_xh)\n",
        "        hy_acc = np.zeros_like(self.W_hy)\n",
        "        bh_acc = np.zeros_like(self.b_h)\n",
        "        by_acc = np.zeros_like(self.b_y)\n",
        "        e_acc = np.zeros_like(self.E)\n",
        "\n",
        "\n",
        "\n",
        "        # this was the tricky to understand. when propagating the error signals to the hidden state at time step t, remember to propagate the error from the future hidden state also.\n",
        "        dh_next = np.zeros_like(self.h)\n",
        "\n",
        "        # start from the last output,\n",
        "        # accumulate the gradients of the hidden layer.\n",
        "        for idx in reversed(range(len(true_seq))):\n",
        "          y_pred = self.y_phases[idx] # the last most output\n",
        "          y_true = true_seq[idx] # the last most true seq\n",
        "          dy = y_pred - y_true # the gradient of the output\n",
        "          by_acc += dy # accumulate the gradient of the bias\n",
        "          ht = self.h_phases[idx+1]\n",
        "          x_t = self.x_phases[idx]\n",
        "          ht_prev = self.h_phases[idx]\n",
        "          dht = np.dot(self.W_hy.T, dy) * (1 - ht**2) + dh_next\n",
        "          dh_next = dht\n",
        "          hh_acc += np.dot(dht, ht_prev.T)\n",
        "          hy_acc += np.dot(dy, ht.T)\n",
        "          bh_acc += dht\n",
        "          xh_acc += np.dot(dht, x_t.T)\n",
        "          dx = np.dot(self.W_xh.T, dht)\n",
        "          embed_index = np.argmax(input_seq[idx])\n",
        "          e_acc[embed_index] += dx.flatten()\n",
        "          # clear\n",
        "\n",
        "        self.W_xh -= alpha * xh_acc\n",
        "        self.W_hh -= alpha * hh_acc\n",
        "        self.W_hy -= alpha * hy_acc\n",
        "        self.b_h -= alpha * bh_acc\n",
        "        self.b_y -= alpha * by_acc\n",
        "        self.E -= alpha * e_acc\n",
        "\n",
        "        # Clear phases\n",
        "        self.h_phases = [np.zeros_like(self.h)]\n",
        "        self.y_phases = []\n",
        "        self.x_phases = []\n",
        "\n",
        "    def save(self, filename=\"rnn_model.pkl\"):\n",
        "      model_data = {\n",
        "          \"W_xh\": self.W_xh,\n",
        "          \"W_hh\": self.W_hh,\n",
        "          \"W_hy\": self.W_hy,\n",
        "          \"b_h\": self.b_h,\n",
        "          \"b_y\": self.b_y,\n",
        "          \"E\": self.E,\n",
        "          \"h\": self.h,\n",
        "          \"input_size\": self.W_xh.shape[1],\n",
        "          \"hidden_state_size\": self.W_xh.shape[0],\n",
        "          \"output_size\": self.W_hy.shape[0]\n",
        "      }\n",
        "      with open(filename, \"wb\") as f:\n",
        "        pickle.dump(model_data, f)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(filename=\"rnn_model.pkl\"):\n",
        "      with open(filename, \"rb\") as f:\n",
        "        model_data = pickle.load(f)\n",
        "\n",
        "      rnn = Rnn(model_data[\"input_size\"], model_data[\"hidden_state_size\"], model_data[\"output_size\"])\n",
        "      rnn.W_xh = model_data[\"W_xh\"]\n",
        "      rnn.W_hh = model_data[\"W_hh\"]\n",
        "      rnn.W_hy = model_data[\"W_hy\"]\n",
        "      rnn.b_h = model_data[\"b_h\"]\n",
        "      rnn.b_y = model_data[\"b_y\"]\n",
        "      rnn.E = model_data[\"E\"]\n",
        "      rnn.h = model_data[\"h\"]\n",
        "      return rnn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPMyCGTv8eXs"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGO6swBRB4qJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "drive_path = '/content/drive/MyDrive/stored_weights'\n",
        "if not os.path.exists(drive_path):\n",
        "  os.makedirs(drive_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbi-LjYtXJB_"
      },
      "source": [
        "Got tired of trying to find a dataset for python code, will just create mine using github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9_ENilkXOUt"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/psf/requests.git\n",
        "print(\"Repository cloned!\")\n",
        "!ls -F # Verify 'requests/' directory exists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnX1aW_NXnO5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "codebase_root_dir = 'requests'\n",
        "output_dataset_file = 'requests_code_dataset.txt'\n",
        "\n",
        "print(f\"Collecting Python files from '{codebase_root_dir}' and saving to '{output_dataset_file}'...\")\n",
        "\n",
        "with open(output_dataset_file, 'w', encoding='utf-8') as outfile:\n",
        "    # os.walk generates the file names in a directory tree by walking the tree\n",
        "    for dirpath, _, filenames in os.walk(codebase_root_dir):\n",
        "        for f in filenames:\n",
        "            if f.endswith('.py'):\n",
        "                file_path = os.path.join(dirpath, f)\n",
        "                try:\n",
        "                    with open(file_path, 'r', encoding='utf-8') as infile:\n",
        "                        outfile.write(infile.read())\n",
        "                        outfile.write(\"\\n\\n# --- FILE_SEPARATOR ---\\n\\n\")\n",
        "                except UnicodeDecodeError:\n",
        "                    print(f\"Skipping {file_path} due to encoding error (likely non-UTF-8 characters).\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Could not read {file_path}: {e}\")\n",
        "\n",
        "print(f\"\\nDataset created: '{output_dataset_file}'\")\n",
        "print(f\"Total size of generated dataset file: {os.path.getsize(output_dataset_file) / (1024*1024):.2f} MB\")\n",
        "\n",
        "# Read the generated dataset into your 'text' variable for the RNN\n",
        "with open(output_dataset_file, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f\"Total characters in the loaded dataset: {len(text)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vs0kQEB1wOE7"
      },
      "outputs": [],
      "source": [
        "print(len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJ0yQ_DiwS2F"
      },
      "outputs": [],
      "source": [
        "text = text[:50000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyF0pRy0ac2Y"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
        "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "print(f\"Vocabulary size: {vocab_size}, characters: {''.join(chars)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU7Cy1FObek4"
      },
      "outputs": [],
      "source": [
        "print(char_to_ix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmJU6AR0eaj3"
      },
      "outputs": [],
      "source": [
        "input_size = 128\n",
        "hidden_state_size = 128\n",
        "output_size = vocab_size\n",
        "rnn = Rnn(input_size, hidden_state_size, output_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-ECbSIIem0A"
      },
      "outputs": [],
      "source": [
        "seq_length = 30       # Number of characters in each training sequence chunk\n",
        "learning_rate = 0.01\n",
        "num_epochs = 100     # Number of times to iterate during training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9wxd34jie8aE"
      },
      "outputs": [],
      "source": [
        "# will crash your ram.\n",
        "\n",
        "# training_data_pairs = []\n",
        "# for i in range(0, len(text) - seq_length):\n",
        "#     input_chunk_chars = text[i : i + seq_length]\n",
        "#     target_chunk_chars = text[i + 1 : i + seq_length + 1] # Shifted by one for next char prediction\n",
        "\n",
        "#     input_one_hots_sequence = []\n",
        "#     target_one_hots_sequence = []\n",
        "\n",
        "#     for char_in, char_target in zip(input_chunk_chars, target_chunk_chars):\n",
        "#         input_one_hot_seq = np.zeros((vocab_size, 1))\n",
        "#         input_one_hot_seq[char_to_ix[char_in]] = 1\n",
        "#         input_one_hots_sequence.append(input_one_hot_seq)\n",
        "#         target_one_hot_seq = np.zeros((vocab_size,1))\n",
        "#         target_one_hot_seq[char_to_ix[char_target]] = 1\n",
        "#         target_one_hots_sequence.append(target_one_hot_seq)\n",
        "\n",
        "#     training_data_pairs.append((input_one_hots_sequence, target_one_hots_sequence))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IwI0PtjXtHGS"
      },
      "outputs": [],
      "source": [
        "# --- Generator Function ---\n",
        "def create_training_data_generator(text_data, sequence_length, char_to_ix, vocab_size):\n",
        "    for i in range(0, len(text_data) - sequence_length):\n",
        "        input_chunk_chars = text_data[i : i + sequence_length]\n",
        "        target_chunk_chars = text_data[i + 1 : i + sequence_length + 1]\n",
        "\n",
        "        input_one_hots_sequence = []\n",
        "        target_one_hots_sequence = []\n",
        "\n",
        "        for char_in, char_target in zip(input_chunk_chars, target_chunk_chars):\n",
        "            input_oh = np.zeros((vocab_size, 1))\n",
        "            input_oh[char_to_ix[char_in]] = 1\n",
        "            input_one_hots_sequence.append(input_oh)\n",
        "\n",
        "            target_oh = np.zeros((vocab_size, 1))\n",
        "            target_oh[char_to_ix[char_target]] = 1\n",
        "            target_one_hots_sequence.append(target_oh)\n",
        "\n",
        "        yield (input_one_hots_sequence, target_one_hots_sequence) # Use yield instead of append\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n--- Starting Training with Generator ---\")\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    data_generator_obj = create_training_data_generator(text, sequence_length=seq_length, char_to_ix=char_to_ix, vocab_size=vocab_size)\n",
        "\n",
        "    for seq_idx, (input_seq_data, target_seq_data) in enumerate(data_generator_obj):\n",
        "        rnn.learn(trueSeq=target_seq_data, alpha=learning_rate, inputSeq=input_seq_data, number_of_iterations=1)\n",
        "\n",
        "\n",
        "# file path /content/drive/MyDrive/stored_weights\n",
        "\n",
        "rnn.save(filename=\"/content/drive/MyDrive/stored_weights/rnn_model.pkl\")\n",
        "print(\"\\n--- Model Saved ---\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbH1-NcbJmSk"
      },
      "outputs": [],
      "source": [
        "# load a pkl file\n",
        "\n",
        "rnn_loaded = Rnn.load(filename=\"/content/drive/MyDrive/stored_weights/rnn_model.pkl\")\n",
        "print(rnn_loaded.b_h)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"rnn_loaded.W_xh (first few values):\", rnn_loaded.W_xh.flatten()[:5])\n",
        "print(\"rnn_loaded.W_hh (first few values):\", rnn_loaded.W_hh.flatten()[:5])\n",
        "print(\"rnn_loaded.W_hy (first few values):\", rnn_loaded.W_hy.flatten()[:5])\n",
        "print(\"rnn_loaded.b_h (first few values):\", rnn_loaded.b_h.flatten()[:5])\n",
        "print(\"rnn_loaded.b_y (first few values):\", rnn_loaded.b_y.flatten()[:5])\n",
        "print(\"rnn_loaded.h (first few values):\", rnn_loaded.h.flatten()[:5]) # Check initial hidden state too"
      ],
      "metadata": {
        "id": "WpbvHIoIaKxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM6R-lPQj64V"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, seed_text, num_chars_to_generate, char_to_ix, ix_to_char, hidden_size):\n",
        "    generated_text_chars = list(seed_text)\n",
        "    model.h = np.zeros((hidden_size, 1), dtype=np.float32 if hasattr(model, 'W_hh') and model.W_hh.dtype == np.float32 else np.float64)\n",
        "    for char_in_seed in seed_text:\n",
        "        idx_in = char_to_ix.get(char_in_seed, 0)\n",
        "        x = model.embed_word(idx_in)\n",
        "        _ = model.forward(x)\n",
        "    if seed_text:\n",
        "        last_char_idx = char_to_ix.get(seed_text[-1], 0)\n",
        "    else:\n",
        "        last_char_idx = char_to_ix.get(' ', 0) if ' ' in char_to_ix else 0\n",
        "    for _ in range(num_chars_to_generate):\n",
        "        x = model.embed_word(last_char_idx)\n",
        "        output_probs = model.forward(x)\n",
        "        p = output_probs.ravel()\n",
        "        p /= p.sum()\n",
        "\n",
        "        next_char_idx = np.random.choice(len(p), p=p)\n",
        "\n",
        "        next_char = ix_to_char[next_char_idx]\n",
        "        generated_text_chars.append(next_char)\n",
        "\n",
        "        last_char_idx = next_char_idx\n",
        "\n",
        "    return \"\".join(generated_text_chars)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JrXFZMdkj5t"
      },
      "outputs": [],
      "source": [
        "seed_text_example = \"def my_function(self, arg):\"\n",
        "num_chars_to_generate_example = 20\n",
        "\n",
        "print(f\"\\n--- Generating text with seed: '{seed_text_example}' ---\")\n",
        "generated_code = generate_text(\n",
        "    model=rnn_loaded,\n",
        "    seed_text=seed_text_example,\n",
        "    num_chars_to_generate=num_chars_to_generate_example,\n",
        "    char_to_ix=char_to_ix,\n",
        "    ix_to_char=ix_to_char,\n",
        "    hidden_size=hidden_state_size\n",
        ")\n",
        "\n",
        "print(generated_code)\n",
        "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "\n",
        "seed_text_example_2 = \"import numpy as \"\n",
        "generated_code_2 = generate_text(rnn, seed_text_example_2, 150, char_to_ix, ix_to_char, hidden_size=hidden_state_size)\n",
        "print(f\"Generated text with seed: '{seed_text_example_2}':\\n{generated_code_2}\")\n",
        "\n",
        "\n",
        "seed_text_example_3 = \"    \"\n",
        "generated_code_3 = generate_text(rnn, seed_text_example_3, 100, char_to_ix, ix_to_char, hidden_size=hidden_state_size)\n",
        "print(f\"Generated text with seed: '{seed_text_example_3}':\\n{generated_code_3}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPbuy4UY9QlS6YdjH5h6dZB"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}